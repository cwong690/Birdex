{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the images and their annotations\n",
    "# image_identifiers = list(image_paths.keys())\n",
    "# random.shuffle(image_identifiers) \n",
    "# for image_id in image_identifiers:\n",
    "#     image_path = image_paths[image_id]\n",
    "#     image = plt.imread(image_path)\n",
    "#     bbox = image_bboxes[image_id]\n",
    "#     parts = image_parts[image_id]\n",
    "#     class_label = image_class_labels[image_id]\n",
    "#     class_name = class_names[class_label]\n",
    "      \n",
    "#     # Render the image\n",
    "#     plt.close(1)\n",
    "#     fig = plt.figure(1, figsize=(16, 12))\n",
    "#     plt.imshow(image)\n",
    "#     plt.title(\"Image ID: %s \\n Class Label: %s  [ID: %s]\" % (image_id, class_name, class_label))\n",
    "      \n",
    "#     # Render the bounding box annotations\n",
    "#     bbox_x, bbox_y, bbox_width, bbox_height = bbox\n",
    "#     currentAxis = plt.gca()\n",
    "#     currentAxis.add_patch(plt.Rectangle((bbox_x , bbox_y), bbox_width, bbox_height, facecolor=\"green\", alpha=0.3))\n",
    "      \n",
    "#     # Render the part annotations\n",
    "#     if has_networkx:\n",
    "#     # Use networkx spring layout\n",
    "        \n",
    "#         G = nx.Graph()\n",
    "#         part_data = []\n",
    "#         initial_positions = {}\n",
    "#         for part_id, part in enumerate(parts):\n",
    "#             x, y, v = part\n",
    "#             if v:\n",
    "#                 part_str = 'part_%d' % (part_id,)\n",
    "#                 label_str = 'label_%d' % (part_id,)\n",
    "            \n",
    "#                 G.add_node(part_str)\n",
    "#                 G.add_node(label_str)\n",
    "#                 G.add_edge(part_str, label_str)\n",
    "            \n",
    "#                 part_data.append(part_str)\n",
    "            \n",
    "#                 initial_positions[part_str] = (x, y)\n",
    "#                 initial_positions[label_str] = (x, y)\n",
    "            \n",
    "#             positions = nx.spring_layout(G, dim=2, k=20.0, pos=initial_positions, fixed=part_data)\n",
    "        \n",
    "#             for part_id, part in enumerate(parts):\n",
    "#                 x, y, v = part\n",
    "#                 if v:\n",
    "#                     plt.plot(x, y, 'o')\n",
    "            \n",
    "#                     label_str = 'label_%d' % (part_id,)\n",
    "#                     label_position = positions[label_str]\n",
    "#                     label_xy = (label_position[0] * bbox_width + bbox_x, label_position[1] * bbox_height + bbox_y)\n",
    "#                     plt.annotate(\n",
    "#                       part_names[part_id], \n",
    "#                       xy=(x, y),\n",
    "#                       xycoords='data', \n",
    "#                       xytext= label_xy, \n",
    "#                       textcoords='data',\n",
    "#                       ha='right', \n",
    "#                       va='bottom',\n",
    "#                       bbox=dict(boxstyle= 'round,pad=0.5', \n",
    "#                             fc='yellow', \n",
    "#                             alpha=0.5),\n",
    "#                       arrowprops=dict(arrowstyle='->', \n",
    "#                               connectionstyle='arc3,rad=0')\n",
    "#                     )\n",
    "    \n",
    "#                 else:\n",
    "#                     # just a basic label annotation for the part locations\n",
    "#                     offset = (2 * math.pi) / len(parts)\n",
    "#                     for part_id, part in enumerate(parts):\n",
    "#                         x, y, v = part\n",
    "#                         if v: \n",
    "#                             # try to offset the part labels so that they don't overlap too badly\n",
    "#                             dist = random.randint(35, 65)\n",
    "#                             offset_x = dist * math.cos(offset * part_id)\n",
    "#                             offset_y = dist * math.sin(offset * part_id)\n",
    "          \n",
    "#                             plt.plot(x, y, 'o')\n",
    "#                             plt.annotate(\n",
    "#                               part_names[part_id], \n",
    "#                               xy=(x, y), \n",
    "#                               xytext=(offset_x, offset_y), \n",
    "#                               textcoords='offset points', \n",
    "#                               ha='right', \n",
    "#                               va='bottom',\n",
    "#                               bbox=dict(boxstyle= 'round,pad=0.5', \n",
    "#                                 fc='yellow', \n",
    "#                                 alpha=0.5),\n",
    "#                               arrowprops=dict(arrowstyle='->', \n",
    "#                                   connectionstyle='arc3,rad=0')\n",
    "#                             )\n",
    "  \n",
    "#     plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the JSON data using json.load()\n",
    "# file = 'animalandveterinary-event-0001-of-0001.json'\n",
    "# with open(file) as train_file:\n",
    "#     dict_train = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting json dataset from dictionary to dataframe\n",
    "# original = pd.DataFrame.from_dict(dict_train['results'])\n",
    "# train = original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingredients = []\n",
    "# drug_name = []\n",
    "# manufacturer = []\n",
    "\n",
    "# for x in train['drug']:\n",
    "#     try:\n",
    "#         ing = x[0]['active_ingredients'][0]['name']\n",
    "#         ingredients.append(ing)\n",
    "#     except:\n",
    "#         continue\n",
    "#     try:\n",
    "#         drug_name.append(x[0]['brand_name'])\n",
    "#     except:\n",
    "#         continue\n",
    "#     try:\n",
    "#         manufacturer.append(x[0]['manufacturer']['name'])\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(np.array(manufacturer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(np.unique(np.array(ingredients)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in range(len(train)):\n",
    "#     reaction = []\n",
    "#     col = train.iloc[x]['reaction']\n",
    "#     for i in range(len(col)):\n",
    "#         reaction.append(col[i]['veddra_term_name'])\n",
    "#     train.iloc[x]['reaction'] = reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.drop(['secondary_reporter', 'primary_reporter', \n",
    "#             'number_of_animals_affected', 'number_of_animals_treated', 'report_id', 'duration', \n",
    "#             'time_between_exposure_and_onset'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # report id is not actually unique. can be dropped.\n",
    "# print(len(train['report_id']))\n",
    "# print(len(train['report_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # unique_aer_id_number is a primary key\n",
    "# print(len(train['unique_aer_id_number']))\n",
    "# print(len(train['unique_aer_id_number'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "def resize_multiple_images(src_folder, dst_path):\n",
    "    # src_folder is the location where images are saved\n",
    "    for dirpath, dirnames, filenames in os.walk(src_folder):\n",
    "        for filename in os.listdir(dirpath):\n",
    "            try:\n",
    "                img=Image.open(filename+'.jpg')\n",
    "                new_img = img.resize((64,64))\n",
    "                if not os.path.exists(dst_path):\n",
    "                    os.makedirs(dst_path)\n",
    "                new_img.save(dst_path+'/'+filename)\n",
    "                print('Resized and saved {}.'.format(filename))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "# src_folder = 'images'\n",
    "# dst_path = 'resized_images'\n",
    "# resize_multiple_images(src_folder, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dirpath, dirnames, filenames in os.walk('images'):\n",
    "#     for filename in os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_multiple_files(path,obj):\n",
    "\n",
    "    i=0\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        try:\n",
    "            f,extension = os.path.splitext(path+filename)\n",
    "            src=path+'/'+filename\n",
    "            dst=path+'/'+obj+str(i)+extension\n",
    "            os.rename(src,dst)\n",
    "            i+=1\n",
    "            print('Rename successful.')\n",
    "        except:\n",
    "            i+=1\n",
    "\n",
    "# path='training_set'\n",
    "# obj=f\"{src_path.split('.')[1]}\"\n",
    "# rename_multiple_files(path,obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def get_data(path):\n",
    "    all_images_as_array=[]\n",
    "    label=[]\n",
    "    for filename in os.listdir(path):\n",
    "        try:\n",
    "            if re.match(r'Black_footed_Albatross',filename):\n",
    "                label.append(1)\n",
    "            else:\n",
    "                label.append(0)\n",
    "            img=Image.open(path + '/'+ filename)\n",
    "            np_array = np.asarray(img)\n",
    "            l,b,c = np_array.shape\n",
    "            np_array = np_array.reshape(l*b*c,)\n",
    "            all_images_as_array.append(np_array)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return np.array(all_images_as_array), np.array(label)\n",
    "\n",
    "# path_to_train_set = 'train_set/'\n",
    "# path_to_test_set = 'test_set/'\n",
    "# X_train,y_train = get_data(path_to_train_set)\n",
    "# X_test, y_test = get_data(path_to_test_set)\n",
    "\n",
    "# print('X_train set : ',X_train)\n",
    "# print('y_train set : ',y_train)\n",
    "# print('X_test set : ',X_test)\n",
    "# print('y_test set : ',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import networkx as nx\n",
    "\n",
    "def load_bounding_box_annotations(dataset_path=''):\n",
    "    bboxes = {}\n",
    "    with open(os.path.join(dataset_path, 'bounding_boxes.txt')) as f:\n",
    "        for line in f:\n",
    "            pieces = line.strip().split()\n",
    "            image_id = pieces[0]\n",
    "            bbox = map(int, pieces[1:])\n",
    "            bboxes[image_id] = bbox\n",
    "    return bboxes\n",
    "\n",
    "def load_part_annotations(dataset_path=''):\n",
    "    parts = {}\n",
    "    with open(os.path.join(dataset_path, 'parts/part_locs.txt')) as f:\n",
    "        for line in f:\n",
    "            pieces = line.strip().split()\n",
    "            image_id = pieces[0]\n",
    "            parts.setdefault(image_id, [0] * 11)\n",
    "            part_id = int(pieces[1])\n",
    "            parts[image_id][part_id] = [int(x) for x in pieces[2:]]\n",
    "    return parts  \n",
    "  \n",
    "def load_part_names(dataset_path=''):\n",
    "    names = {}\n",
    "    with open(os.path.join(dataset_path, 'parts/parts.txt')) as f:\n",
    "        for line in f:\n",
    "            pieces = line.strip().split()\n",
    "            part_id = int(pieces[0])\n",
    "            names[part_id] = ' '.join(pieces[1:])\n",
    "    return names  \n",
    "    \n",
    "def load_class_names(dataset_path=''):\n",
    "    names = {}\n",
    "    with open(os.path.join(dataset_path, 'classes.txt')) as f:\n",
    "        for line in f:\n",
    "            pieces = line.strip().split()\n",
    "            class_id = pieces[0]\n",
    "            names[class_id] = ' '.join(pieces[1:])\n",
    "    return names\n",
    "\n",
    "def load_image_labels(dataset_path=''):\n",
    "    labels = {}\n",
    "    with open(os.path.join(dataset_path, 'image_class_labels.txt')) as f:\n",
    "        for line in f:\n",
    "            pieces = line.strip().split()\n",
    "            image_id = pieces[0]\n",
    "            class_id = pieces[1]\n",
    "            labels[image_id] = class_id\n",
    "    return labels\n",
    "        \n",
    "def load_image_paths(dataset_path='', path_prefix=''):\n",
    "    paths = {}\n",
    "    with open(os.path.join(dataset_path, 'images.txt')) as f:\n",
    "        for line in f:\n",
    "            pieces = line.strip().split()\n",
    "            image_id = pieces[0]\n",
    "            path = os.path.join(path_prefix, pieces[1])\n",
    "            paths[image_id] = path\n",
    "    return paths\n",
    "\n",
    "def load_image_sizes(dataset_path=''):\n",
    "    sizes = {}\n",
    "    with open(os.path.join(dataset_path, 'sizes.txt')) as f:\n",
    "        for line in f:\n",
    "            pieces = line.strip().split()\n",
    "            image_id = pieces[0]\n",
    "            width, height = map(int, pieces[1:])\n",
    "            sizes[image_id] = [width, height]\n",
    "    return sizes\n",
    "\n",
    "def load_hierarchy(dataset_path=''):\n",
    "    parents = {}\n",
    "    with open(os.path.join(dataset_path, 'hierarchy.txt')) as f:\n",
    "        for line in f:\n",
    "            pieces = line.strip().split()\n",
    "            child_id, parent_id = pieces\n",
    "            parents[child_id] = parent_id\n",
    "    return parents\n",
    "\n",
    "def load_photographers(dataset_path=''):\n",
    "    photographers = {}\n",
    "    with open(os.path.join(dataset_path, 'photographers.txt')) as f:\n",
    "        for line in f:\n",
    "            pieces = line.strip().split()\n",
    "            image_id = pieces[0]\n",
    "            photographers[image_id] = ' '.join(pieces[1:])\n",
    "    return photographers\n",
    "\n",
    "def load_train_test_split(dataset_path=''):\n",
    "    train_images = []\n",
    "    test_images = []\n",
    "    with open(os.path.join(dataset_path, 'train_test_split.txt')) as f:\n",
    "        for line in f:\n",
    "            pieces = line.strip().split()\n",
    "            image_id = pieces[0]\n",
    "            is_train = int(pieces[1])\n",
    "            if is_train:\n",
    "                train_images.append(image_id)\n",
    "            else:\n",
    "                test_images.append(image_id)\n",
    "    return train_images, test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'nabirds/'\n",
    "image_path  = 'nabirds/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = load_image_paths(dataset_path, path_prefix=image_path)\n",
    "image_sizes = load_image_sizes(dataset_path)\n",
    "image_bboxes = load_bounding_box_annotations(dataset_path)\n",
    "image_parts = load_part_annotations(dataset_path)\n",
    "image_class_labels = load_image_labels(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = load_class_names(dataset_path)\n",
    "class_hierarchy = load_hierarchy(dataset_path)\n",
    "\n",
    "# Load in the part data\n",
    "part_names = load_part_names(dataset_path)\n",
    "part_ids = part_names.keys()\n",
    "part_ids = sorted(part_ids)\n",
    "\n",
    "# Load in the photographers\n",
    "photographers = load_photographers(dataset_path)\n",
    "\n",
    "# Load in the train / test split\n",
    "train_images, test_images = load_train_test_split(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0001afd4-99a1-4a67-b940-d419413e23b3',\n",
       " '0007181f-a727-4481-ad89-591200c61b9d',\n",
       " '00071e20-8156-4bd8-b5ca-6445c2560ee5',\n",
       " '00081fce-2a74-4a9f-b52b-9bb8871a48e2',\n",
       " '00085a7b-efcc-4c08-a830-38477e749101']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animals = []\n",
    "# with open('wcs_camera_traps.json') as f:\n",
    "#     for i, k in enumerate(f):\n",
    "#         if (i < 100):\n",
    "#             print(k)\n",
    "# #             try:\n",
    "#                 animals.append(json.load(k))\n",
    "#             except:\n",
    "#                 continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37064bitanaconda3virtualenvdc59f3b7c1d64353bf7dcc6d7e32f36c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
